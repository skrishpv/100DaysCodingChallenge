{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUDT758B Lab 1 \n",
    "#### Shashank Puthanveedu (116836982) \n",
    "##### Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "id": "EbVri-6FrK7W",
    "outputId": "a3f268f4-22f3-4c63-cd0f-1911efdb9216"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Heres a single  to add  to Kindle. Just read t...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If you tire of Non-Fiction.. Check out http://...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ghost of Round Island is supposedly nonfiction.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why is Barnes and Nobles version of the Kindle...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Maria:  Do you mean the Nook?  Be careful  bo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  labels\n",
       "0  Heres a single  to add  to Kindle. Just read t...    neutral       1\n",
       "1  If you tire of Non-Fiction.. Check out http://...    neutral       1\n",
       "2   Ghost of Round Island is supposedly nonfiction.     neutral       1\n",
       "3  Why is Barnes and Nobles version of the Kindle...   negative       0\n",
       "4  @Maria:  Do you mean the Nook?  Be careful  bo...   positive       2"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pandas and numpy libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "fname = 'facebook_comments.csv'\n",
    "df_train = pd.read_csv(fname, header = None, names = ['text', 'sentiment'], encoding = 'iso-8859-1', lineterminator = '\\n')\n",
    "\n",
    "# Set labels to create levels in sentiments\n",
    "sent = {'positive':2, 'neutral':1,'negative':0}\n",
    "df_train['labels'] = df_train['sentiment'].str.strip().map(sent)\n",
    "\n",
    "# Get texts and labels\n",
    "training_texts = df_train.text.values\n",
    "labels = df_train.labels.values\n",
    "\n",
    "# See a sample data\n",
    "print(type(training_texts), type(labels))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "0aMcNSt2scwG",
    "outputId": "405b8186-67d6-4868-e47f-fd280e9ce40b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999, 500) , (1999,)\n",
      "[1 1 1 0 2 2 2 0 2 0]\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.28915636 0.         0.         0.\n",
      " 0.         0.         0.2971592  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Import the TfidfVectorizer packahe from sklearn to vectorize the texts.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_features=500, ngram_range=(1,1))\n",
    "instances = vectorizer.fit_transform(training_texts)\n",
    "\n",
    "X = instances.toarray()\n",
    "Y = labels\n",
    "\n",
    "\n",
    "print(X.shape,',',Y.shape)\n",
    "print(Y[:10])\n",
    "print(X[0,:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "n0VMdQzpsd_K",
    "outputId": "13e2a166-6f65-44b2-fc3d-9dbd6becd176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - mean: 64.1332% (std: +/- 2.0919%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set the configuration for kfold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle = True, random_state=2020)\n",
    "# Set the configuration for a Random Forest model\n",
    "rf_model = RandomForestClassifier(criterion='entropy',max_depth = 2, random_state = 2020)\n",
    "rf_cvscores = []\n",
    "\n",
    "# Train the model and calculate the accuracy\n",
    "for train_idx, val_idx in kfold.split(X):\n",
    "  rf_model.fit(X[train_idx], Y[train_idx])\n",
    "  acc = rf_model.score(X[val_idx], Y[val_idx])\n",
    "  rf_cvscores.append(acc)\n",
    "\n",
    "print(\"Random Forest - mean: %.4f%% (std: +/- %.4f%%)\"%(np.mean(rf_cvscores)*100,np.std(rf_cvscores)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "l0WLPkX2zPWV"
   },
   "outputs": [],
   "source": [
    "# Import the required pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "N3s0kjw-z6tK"
   },
   "outputs": [],
   "source": [
    "# Set the configuration/ tunable parameters of the model\n",
    "epochs = 50\n",
    "lr = 0.01\n",
    "indim = X.shape[1]\n",
    "outdim = 3\n",
    "drate = 0.7\n",
    "batch_size = 36\n",
    "\n",
    "# Create tensors\n",
    "X_tensor = torch.from_numpy(X)\n",
    "Y_tensor = torch.from_numpy(Y)\n",
    "\n",
    "# Create a Dataset\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "train_size = int(0.8*len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Split the dataset into train and validation dataset\n",
    "train_loader = DataLoader(train_dataset, shuffle = True, batch_size = batch_size)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size = batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "rpWJw4hG0eeP",
    "outputId": "035d6752-20f7-4816-ac2b-143104c17205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentNetwork(\n",
      "  (fc1): Linear(in_features=500, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (do1): Dropout(p=0.7, inplace=False)\n",
      "  (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build the Model Network Structure: Linear -> Relu -> Batch Normalization -> Dropout -> ... -> Log_Softmax\n",
    "class SentimentNetwork(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, dropout_rate):\n",
    "    super(SentimentNetwork, self).__init__()\n",
    "    self.fc1 = nn.Linear(input_dim, 100, bias = True)\n",
    "    self.fc2 = nn.Linear(100, 50, bias = True)\n",
    "    self.fc3 = nn.Linear(50, output_dim, bias = True)\n",
    "    self.do1 = nn.Dropout(dropout_rate)\n",
    "    self.bn1 = nn.BatchNorm1d(100)\n",
    "    self.bn2 = nn.BatchNorm1d(50)\n",
    "  def forward(self,x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.bn1(x)\n",
    "    x = self.do1(x)\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.bn2(x)\n",
    "    x = self.do1(x)\n",
    "    x = F.log_softmax(self.fc3( x))\n",
    "    return x\n",
    "\n",
    "model = SentimentNetwork(indim, outdim, drate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v57qlFjLGoJS"
   },
   "outputs": [],
   "source": [
    "# Using an Adam optimizer for adaptive learning rate\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(),lr = lr, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(),lr = lr)\n",
    "\n",
    "# Set CrossEntropyLoss as the loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Function to calculate the accuracy for each batch within each epoch\n",
    "def accuracy(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JcUw4BZ50hfL"
   },
   "outputs": [],
   "source": [
    "# define a training process for the model\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "  epoch_loss, epoch_acc = 0.0,0.0\n",
    "  loss,acc=0,0\n",
    "  model.train()\n",
    " \n",
    "  for batch_x, batch_y in train_loader:\n",
    "    #zero gradient\n",
    "    optimizer.zero_grad()\n",
    "    #predictions= calculates the predicted output for the current batch batch_x\n",
    "    model_out=model(batch_x.float())\n",
    "    #loss= loass for the current batch using predicions ans batch_y\n",
    "    loss=criterion(model_out,batch_y)\n",
    "    #acc= calculates the acc using predictions (batch_size X output_dim) and batch_y (batch_size X 1)\n",
    "    train_acc = accuracy(model_out, batch_y)\n",
    "    #backpropgate \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate the epoch loss and accuracy\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_acc += train_acc.item()\n",
    "  return epoch_loss/len(train_loader), epoch_acc/len(train_loader)\n",
    " \n",
    "# define a validation/evaluation process function\n",
    "def evaluate(model, val_loader, criterion):\n",
    "  epoch_loss, epoch_acc = 0.0,0.0\n",
    "  loss,acc=0,0\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch_x, batch_y in val_loader:\n",
    "      #predictions= calculates the predicted output for the current batch batch_x\n",
    "      model_out=model(batch_x.float())\n",
    "      #loss= loass for the current batch using predicions ans batch_y\n",
    "      loss=criterion(model_out,batch_y)\n",
    "      #acc= calculates the acc using predictions (batch_size X output_dim) and batch_y (batch_size X 1)\n",
    "      val_acc = accuracy(model_out, batch_y)\n",
    "\n",
    "      # Calculate the epoch loss and accuracy    \n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += val_acc.item()\n",
    "  return epoch_loss/len(val_loader), epoch_acc/len(val_loader)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Up3AEhWMGmMs",
    "outputId": "e288806c-0746-4a94-f7db-b0bdc69ccdc8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.9618 | Train Acc: 0.5763\n",
      "\t Val. Loss: 0.7352 |  Val. Acc: 0.6389\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.6613 | Train Acc: 0.7243\n",
      "\t Val. Loss: 0.4773 |  Val. Acc: 0.8125\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.5341 | Train Acc: 0.8049\n",
      "\t Val. Loss: 0.4661 |  Val. Acc: 0.8009\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.4966 | Train Acc: 0.8023\n",
      "\t Val. Loss: 0.4016 |  Val. Acc: 0.8611\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.4822 | Train Acc: 0.8169\n",
      "\t Val. Loss: 0.3821 |  Val. Acc: 0.8449\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.4205 | Train Acc: 0.8441\n",
      "\t Val. Loss: 0.3738 |  Val. Acc: 0.8403\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.3938 | Train Acc: 0.8499\n",
      "\t Val. Loss: 0.2998 |  Val. Acc: 0.8819\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.3741 | Train Acc: 0.8553\n",
      "\t Val. Loss: 0.3007 |  Val. Acc: 0.8866\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.3430 | Train Acc: 0.8805\n",
      "\t Val. Loss: 0.3416 |  Val. Acc: 0.8681\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.3144 | Train Acc: 0.8865\n",
      "\t Val. Loss: 0.2857 |  Val. Acc: 0.8981\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.2980 | Train Acc: 0.8933\n",
      "\t Val. Loss: 0.2651 |  Val. Acc: 0.9120\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.2742 | Train Acc: 0.9067\n",
      "\t Val. Loss: 0.2372 |  Val. Acc: 0.9236\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.2820 | Train Acc: 0.8943\n",
      "\t Val. Loss: 0.2515 |  Val. Acc: 0.9097\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.2557 | Train Acc: 0.9091\n",
      "\t Val. Loss: 0.2504 |  Val. Acc: 0.9236\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.2279 | Train Acc: 0.9241\n",
      "\t Val. Loss: 0.2381 |  Val. Acc: 0.9329\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.2405 | Train Acc: 0.9210\n",
      "\t Val. Loss: 0.3261 |  Val. Acc: 0.9120\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.2299 | Train Acc: 0.9204\n",
      "\t Val. Loss: 0.2266 |  Val. Acc: 0.9329\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.2189 | Train Acc: 0.9267\n",
      "\t Val. Loss: 0.3068 |  Val. Acc: 0.8935\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.2335 | Train Acc: 0.9267\n",
      "\t Val. Loss: 0.2990 |  Val. Acc: 0.9213\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.2255 | Train Acc: 0.9214\n",
      "\t Val. Loss: 0.2337 |  Val. Acc: 0.9282\n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.2352 | Train Acc: 0.9190\n",
      "\t Val. Loss: 0.2502 |  Val. Acc: 0.9167\n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.2129 | Train Acc: 0.9278\n",
      "\t Val. Loss: 0.2832 |  Val. Acc: 0.9213\n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.1772 | Train Acc: 0.9384\n",
      "\t Val. Loss: 0.2166 |  Val. Acc: 0.9329\n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.1710 | Train Acc: 0.9451\n",
      "\t Val. Loss: 0.2053 |  Val. Acc: 0.9398\n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.1818 | Train Acc: 0.9360\n",
      "\t Val. Loss: 0.1938 |  Val. Acc: 0.9398\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.1750 | Train Acc: 0.9377\n",
      "\t Val. Loss: 0.2242 |  Val. Acc: 0.9329\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.1728 | Train Acc: 0.9460\n",
      "\t Val. Loss: 0.2195 |  Val. Acc: 0.9375\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.1808 | Train Acc: 0.9446\n",
      "\t Val. Loss: 0.2659 |  Val. Acc: 0.9144\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.1901 | Train Acc: 0.9325\n",
      "\t Val. Loss: 0.1959 |  Val. Acc: 0.9398\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.1649 | Train Acc: 0.9442\n",
      "\t Val. Loss: 0.2664 |  Val. Acc: 0.9236\n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.1581 | Train Acc: 0.9522\n",
      "\t Val. Loss: 0.2031 |  Val. Acc: 0.9398\n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.1503 | Train Acc: 0.9506\n",
      "\t Val. Loss: 0.2106 |  Val. Acc: 0.9421\n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.1632 | Train Acc: 0.9442\n",
      "\t Val. Loss: 0.2103 |  Val. Acc: 0.9468\n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.1669 | Train Acc: 0.9526\n",
      "\t Val. Loss: 0.2205 |  Val. Acc: 0.9421\n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.1656 | Train Acc: 0.9417\n",
      "\t Val. Loss: 0.1996 |  Val. Acc: 0.9421\n",
      "Epoch: 36\n",
      "\tTrain Loss: 0.1658 | Train Acc: 0.9485\n",
      "\t Val. Loss: 0.1956 |  Val. Acc: 0.9444\n",
      "Epoch: 37\n",
      "\tTrain Loss: 0.1709 | Train Acc: 0.9444\n",
      "\t Val. Loss: 0.1919 |  Val. Acc: 0.9444\n",
      "Epoch: 38\n",
      "\tTrain Loss: 0.1435 | Train Acc: 0.9553\n",
      "\t Val. Loss: 0.2069 |  Val. Acc: 0.9444\n",
      "Epoch: 39\n",
      "\tTrain Loss: 0.1362 | Train Acc: 0.9549\n",
      "\t Val. Loss: 0.2108 |  Val. Acc: 0.9444\n",
      "Epoch: 40\n",
      "\tTrain Loss: 0.1462 | Train Acc: 0.9531\n",
      "\t Val. Loss: 0.1907 |  Val. Acc: 0.9444\n",
      "Epoch: 41\n",
      "\tTrain Loss: 0.1306 | Train Acc: 0.9627\n",
      "\t Val. Loss: 0.1976 |  Val. Acc: 0.9491\n",
      "Epoch: 42\n",
      "\tTrain Loss: 0.1651 | Train Acc: 0.9402\n",
      "\t Val. Loss: 0.1941 |  Val. Acc: 0.9468\n",
      "Epoch: 43\n",
      "\tTrain Loss: 0.1384 | Train Acc: 0.9519\n",
      "\t Val. Loss: 0.1930 |  Val. Acc: 0.9468\n",
      "Epoch: 44\n",
      "\tTrain Loss: 0.1830 | Train Acc: 0.9440\n",
      "\t Val. Loss: 0.1926 |  Val. Acc: 0.9375\n",
      "Epoch: 45\n",
      "\tTrain Loss: 0.1625 | Train Acc: 0.9562\n",
      "\t Val. Loss: 0.2332 |  Val. Acc: 0.9236\n",
      "Epoch: 46\n",
      "\tTrain Loss: 0.1511 | Train Acc: 0.9625\n",
      "\t Val. Loss: 0.2045 |  Val. Acc: 0.9491\n",
      "Epoch: 47\n",
      "\tTrain Loss: 0.1348 | Train Acc: 0.9623\n",
      "\t Val. Loss: 0.1887 |  Val. Acc: 0.9491\n",
      "Epoch: 48\n",
      "\tTrain Loss: 0.1303 | Train Acc: 0.9580\n",
      "\t Val. Loss: 0.1749 |  Val. Acc: 0.9537\n",
      "Epoch: 49\n",
      "\tTrain Loss: 0.1118 | Train Acc: 0.9691\n",
      "\t Val. Loss: 0.2058 |  Val. Acc: 0.9491\n",
      "Epoch: 50\n",
      "\tTrain Loss: 0.1324 | Train Acc: 0.9599\n",
      "\t Val. Loss: 0.2099 |  Val. Acc: 0.9444\n"
     ]
    }
   ],
   "source": [
    "# Train and validate the model\n",
    "for epoch in range(epochs):\n",
    "  best_valid_loss = 9999\n",
    "  # Calculate the training loss and accuracy\n",
    "  train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "  # Calculate the validation loss and accuracy\n",
    "  valid_loss, valid_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "  # Save the weights with least loss for future use\n",
    "  if valid_loss < best_valid_loss:\n",
    "    best_valid_loss = valid_loss\n",
    "    torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "\n",
    "  # Print the loss and accuracy values  \n",
    "  print(f'Epoch: {epoch+1:02}')\n",
    "  print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "  print(f'\\t Val. Loss: {valid_loss:.4f} |  Val. Acc: {valid_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Shashank_Lab1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
