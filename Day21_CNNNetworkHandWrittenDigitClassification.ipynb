{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shashank_Puthanveedu_Lab2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DemfiTtKKpr8"
      },
      "source": [
        "#mount the files to googledriveb"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnMZQXJHLmLe",
        "outputId": "2cf63fd8-60db-4b59-b7b6-32d524722a6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create a CNN for handwriting recognition problem\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # input_image: 1x28x28\n",
        "    # define conv1: 6 filters 5x5\n",
        "    self.conv1 = nn.Conv2d(1,6,5)\n",
        "    # define conv2: 16 filters 5x5\n",
        "    self.conv2 = nn.Conv2d(6,16,(5,5))\n",
        "    # define a maxpooling layer : 2x2\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    # define a sub fully connected feed forward netword after CNN: fc1 120 neurons, fc2 84 neurons, fc3 output\n",
        "    self.fc1=nn.Linear(16*4*4,120)\n",
        "    self.fc2=nn.Linear(120,84)\n",
        "    self.fc3=nn.Linear(84,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # x-->conv1 --> relu -->pooling -->conv2 --> relu -->pooling --> fully connected\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = x.view(-1,self.num_flat_features(x))\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.log_softmax(self.fc3(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "  def num_flat_features(self,x):\n",
        "    size = x.size()[1:]\n",
        "    num_features = 1\n",
        "    for s in size:\n",
        "      num_features *= s\n",
        "    return num_features  \n",
        "\n",
        "net = Net().to(device)\n",
        "print(net)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG1VTbefLmIE"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# create a customized Dataset\n",
        "# override __init__, __len__, and __getitem__ methods\n",
        "\n",
        "class MNISTDataset(Dataset):\n",
        "\n",
        "  def __init__(self, dir, transform = None):\n",
        "    self.dir = dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    files = glob.glob(self.dir+'/*.jpg')[:500] # return all file names in a given folder as a list\n",
        "    return len(files)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    \n",
        "    all_files = glob.glob(self.dir+'/*.jpg')[:500]\n",
        "    img_fname = os.path.join(self.dir,all_files[idx])\n",
        "    image = io.imread(img_fname) # numpy array of that particular image\n",
        "\n",
        "    digit = int(self.dir.split('/')[-1].strip())\n",
        "    label = np.array(digit)\n",
        "\n",
        "    instance = {'image':image, 'label':label}\n",
        "\n",
        "    if self.transform:\n",
        "      instance = self.transform(instance)\n",
        "\n",
        "    return instance"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecPkuC4mLmF1"
      },
      "source": [
        "# create a customized transformation: rescale, crop, totensor, etc...\n",
        "from skimage import transform\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "class Rescale(object):\n",
        "\n",
        "  def __init__(self, output_size):\n",
        "    assert isinstance(output_size, (int,tuple))\n",
        "    self.output_size = output_size\n",
        "\n",
        "  def __call__(self,sample):\n",
        "    image, label = sample['image'], sample['label']\n",
        "\n",
        "    h,w = image.shape[-2:]\n",
        "    if isinstance(self.output_size, int):\n",
        "      if h>w:\n",
        "        new_h, new_w = self.output_size*h/w, self.output_size\n",
        "      else:\n",
        "        new_h, new_w = self.output_size, self.output_size*w/h\n",
        "    else:\n",
        "      new_h, new_w = self.output_size\n",
        "    \n",
        "    new_h, new_w = int(new_h), int(new_w)\n",
        "\n",
        "    new_image = transform.resize(image, (new_h, new_w))\n",
        "    return{'image':new_image, 'label':label}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcQjqolZLmCy"
      },
      "source": [
        "class ToTensor(object):\n",
        "\n",
        "  def __call__(self, sample):\n",
        "\n",
        "    image, label = sample['image'], sample['label']\n",
        "    image = image.reshape((1,image.shape[0], image.shape[1]))\n",
        "    return {'image':torch.from_numpy(image), 'label':torch.from_numpy(label)}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erorUQh1Ll_U",
        "outputId": "eac66f16-0f93-406a-ee94-27b638b1a531",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create training/validation dataset/dataloader\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import transforms\n",
        "\n",
        "batch_size = 48\n",
        "\n",
        "list_datasets = []\n",
        "for i in range(10):\n",
        "  curr_ds = MNISTDataset('/content/drive/My Drive/trainingset/'+str(i),transform =transforms.Compose([Rescale(28), ToTensor()]))\n",
        "  list_datasets.append(curr_ds)\n",
        "\n",
        "dataset = torch.utils.data.ConcatDataset(list_datasets)\n",
        "print(len(dataset))\n",
        "\n",
        "train_size = int(len(dataset)*0.7)\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=1)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size, shuffle=True, num_workers=1)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAfV6j7aLl8k",
        "outputId": "8d693220-1813-4e1c-9e11-b4da1a22e66c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# training and validation\n",
        "import torch.optim as optim\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = 0.01\n",
        "optimizer = optim.Adam(net.parameters(), lr= learning_rate, weight_decay = 1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  for batch_idx, batch in enumerate(train_dataloader):\n",
        "    inputs, targets = batch['image'].to(device, dtype = torch.float), batch['label'].to(device, dtype = torch.long)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    predicted_output = net(inputs)\n",
        "    loss = criterion(predicted_output, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if (batch_idx+1)%10 == 0:\n",
        "      print('epoch: %d, batch: %d, training loss: %.3f'%(epoch+1, batch_idx+1, running_loss/10))\n",
        "      running_loss = 0.0\n",
        "    \n",
        "  net.eval()\n",
        "\n",
        "  correct = [0.0]*10\n",
        "  total = [0.0]*10\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(val_dataloader):\n",
        "      images, labels = batch['image'].to(device, dtype = torch.float), batch['label'].to(device, dtype = torch.long)\n",
        "      predicted_outputs = net(images)\n",
        "\n",
        "      _,predicted_labels = torch.max(predicted_outputs,1)\n",
        "      c = (predicted_labels == labels)\n",
        "\n",
        "      for i in range(len(labels)):\n",
        "        label = labels[i]\n",
        "        correct[label] += c[i].item()\n",
        "        total[label] +=1\n",
        "  \n",
        "  for i in range(10):\n",
        "    print('\\t Validation accuracy for digit: %d, %.2f'% (i, 100*correct[i]/total[i]))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, batch: 10, training loss: 2.172\n",
            "epoch: 1, batch: 20, training loss: 1.207\n",
            "epoch: 1, batch: 30, training loss: 0.746\n",
            "epoch: 1, batch: 40, training loss: 0.532\n",
            "epoch: 1, batch: 50, training loss: 0.473\n",
            "epoch: 1, batch: 60, training loss: 0.455\n",
            "epoch: 1, batch: 70, training loss: 0.445\n",
            "\t Validation accuracy for digit: 0, 90.07\n",
            "\t Validation accuracy for digit: 1, 97.28\n",
            "\t Validation accuracy for digit: 2, 83.77\n",
            "\t Validation accuracy for digit: 3, 87.97\n",
            "\t Validation accuracy for digit: 4, 79.62\n",
            "\t Validation accuracy for digit: 5, 92.26\n",
            "\t Validation accuracy for digit: 6, 99.31\n",
            "\t Validation accuracy for digit: 7, 95.39\n",
            "\t Validation accuracy for digit: 8, 83.66\n",
            "\t Validation accuracy for digit: 9, 90.58\n",
            "epoch: 2, batch: 10, training loss: 0.165\n",
            "epoch: 2, batch: 20, training loss: 0.311\n",
            "epoch: 2, batch: 30, training loss: 0.340\n",
            "epoch: 2, batch: 40, training loss: 0.256\n",
            "epoch: 2, batch: 50, training loss: 0.207\n",
            "epoch: 2, batch: 60, training loss: 0.221\n",
            "epoch: 2, batch: 70, training loss: 0.216\n",
            "\t Validation accuracy for digit: 0, 95.04\n",
            "\t Validation accuracy for digit: 1, 98.64\n",
            "\t Validation accuracy for digit: 2, 83.12\n",
            "\t Validation accuracy for digit: 3, 93.67\n",
            "\t Validation accuracy for digit: 4, 94.27\n",
            "\t Validation accuracy for digit: 5, 97.42\n",
            "\t Validation accuracy for digit: 6, 99.31\n",
            "\t Validation accuracy for digit: 7, 98.68\n",
            "\t Validation accuracy for digit: 8, 95.42\n",
            "\t Validation accuracy for digit: 9, 87.68\n",
            "epoch: 3, batch: 10, training loss: 0.109\n",
            "epoch: 3, batch: 20, training loss: 0.141\n",
            "epoch: 3, batch: 30, training loss: 0.146\n",
            "epoch: 3, batch: 40, training loss: 0.098\n",
            "epoch: 3, batch: 50, training loss: 0.114\n",
            "epoch: 3, batch: 60, training loss: 0.130\n",
            "epoch: 3, batch: 70, training loss: 0.187\n",
            "\t Validation accuracy for digit: 0, 95.74\n",
            "\t Validation accuracy for digit: 1, 99.32\n",
            "\t Validation accuracy for digit: 2, 89.61\n",
            "\t Validation accuracy for digit: 3, 93.04\n",
            "\t Validation accuracy for digit: 4, 94.90\n",
            "\t Validation accuracy for digit: 5, 98.71\n",
            "\t Validation accuracy for digit: 6, 97.24\n",
            "\t Validation accuracy for digit: 7, 87.50\n",
            "\t Validation accuracy for digit: 8, 92.81\n",
            "\t Validation accuracy for digit: 9, 94.20\n",
            "epoch: 4, batch: 10, training loss: 0.122\n",
            "epoch: 4, batch: 20, training loss: 0.111\n",
            "epoch: 4, batch: 30, training loss: 0.108\n",
            "epoch: 4, batch: 40, training loss: 0.112\n",
            "epoch: 4, batch: 50, training loss: 0.158\n",
            "epoch: 4, batch: 60, training loss: 0.090\n",
            "epoch: 4, batch: 70, training loss: 0.113\n",
            "\t Validation accuracy for digit: 0, 89.36\n",
            "\t Validation accuracy for digit: 1, 99.32\n",
            "\t Validation accuracy for digit: 2, 90.91\n",
            "\t Validation accuracy for digit: 3, 90.51\n",
            "\t Validation accuracy for digit: 4, 87.26\n",
            "\t Validation accuracy for digit: 5, 96.13\n",
            "\t Validation accuracy for digit: 6, 100.00\n",
            "\t Validation accuracy for digit: 7, 98.68\n",
            "\t Validation accuracy for digit: 8, 92.81\n",
            "\t Validation accuracy for digit: 9, 93.48\n",
            "epoch: 5, batch: 10, training loss: 0.114\n",
            "epoch: 5, batch: 20, training loss: 0.099\n",
            "epoch: 5, batch: 30, training loss: 0.087\n",
            "epoch: 5, batch: 40, training loss: 0.088\n",
            "epoch: 5, batch: 50, training loss: 0.071\n",
            "epoch: 5, batch: 60, training loss: 0.114\n",
            "epoch: 5, batch: 70, training loss: 0.100\n",
            "\t Validation accuracy for digit: 0, 96.45\n",
            "\t Validation accuracy for digit: 1, 89.12\n",
            "\t Validation accuracy for digit: 2, 92.86\n",
            "\t Validation accuracy for digit: 3, 96.84\n",
            "\t Validation accuracy for digit: 4, 92.99\n",
            "\t Validation accuracy for digit: 5, 99.35\n",
            "\t Validation accuracy for digit: 6, 95.17\n",
            "\t Validation accuracy for digit: 7, 97.37\n",
            "\t Validation accuracy for digit: 8, 95.42\n",
            "\t Validation accuracy for digit: 9, 92.03\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}