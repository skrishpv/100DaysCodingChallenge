{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shashank_Puthanveedu_Lab3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTDMcSrK5qXx"
      },
      "source": [
        "## Lab 3 -- Sentiment Analysis Using RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8IlJgf_1KoH",
        "outputId": "3c7ba784-33db-4a6a-f168-415ced34cef2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbkeA5Km8xDX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzqbTXgf7s_e"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpi1mErA_1Nl"
      },
      "source": [
        "### Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enM2E37W8cxb",
        "outputId": "64e1d416-3c8a-41d3-acbd-2fa232afaea3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device = torch.device ('cuda:0' if torch.cuda.is_available else 'cpu')\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.n_layers = n_layers\n",
        "    self.rnn = nn.RNN(input_size,hidden_size,n_layers,batch_first=True)\n",
        "    self.fc1 = nn.Linear(hidden_size,output_size)\n",
        "    self.fc2 = nn.Linear(output_size,2)\n",
        "\n",
        "  def forward(self,x, hidden):\n",
        "    batch_size = x.size()[0]\n",
        "    hidden = self.init_hidden(batch_size)\n",
        "    \n",
        "    rnn_out,hidden = self.rnn(x,hidden)\n",
        "    rnn_out = self.fc1(rnn_out)\n",
        "    last_out = rnn_out[:,-1,:].view(batch_size,-1)\n",
        "    out = F.softmax(self.fc2(last_out))\n",
        "    return out,hidden \n",
        "\n",
        "  def init_hidden(self,batch_size):\n",
        "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).cuda()\n",
        "    return hidden\n",
        "\n",
        "model = Model(200,32,256,3).to(device)\n",
        "print(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (rnn): RNN(200, 256, num_layers=3, batch_first=True)\n",
            "  (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A77VeAEdAvk6"
      },
      "source": [
        "### Pre-process the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X43_ZrjQA1tG"
      },
      "source": [
        "positive_dir = '/content/drive/My Drive/reviews/pos/'\n",
        "negative_dir = '/content/drive/My Drive/reviews/neg/'\n",
        "posList = os.listdir(positive_dir)\n",
        "negList = os.listdir(negative_dir)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6mvuTN078DO"
      },
      "source": [
        "# Define lists for text corpus and labels \n",
        "sentences = []\n",
        "labels = []\n",
        "# Read positive reviews from the folder and assign labels\n",
        "for sentence in posList[:1000]:\n",
        "  path = positive_dir+sentence\n",
        "  f = open(path,\"r\")\n",
        "  positivetext = f.read().replace('\\n',' ')\n",
        "  sentences.append(positivetext)\n",
        "  labels.append([1,0])\n",
        "  f.close()\n",
        "\n",
        "# Read negative sentences from the folder and assign labels\n",
        "for sentence in negList[:1000]:\n",
        "  path = negative_dir+sentence\n",
        "  f = open(path,\"r\")\n",
        "  negativetext = f.read().replace('\\n',' ')\n",
        "  sentences.append(negativetext)\n",
        "  labels.append([0,1])\n",
        "  f.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGgy3xpMBhLM",
        "outputId": "059bd140-0608-4372-de41-bd87f7aaeff9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=200, stop_words='english',ngram_range=(1,1))\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "Y = np.array(labels)\n",
        "print(X.shape, Y.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 200) (2000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-U-o_WjFRlW",
        "outputId": "1c3e68f2-c316-44b5-9e65-2d985a92e49f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create tokens for each word\n",
        "tokenizer = vectorizer.build_tokenizer()\n",
        "words_token = [tokenizer(word) for word in sentences]\n",
        "\n",
        "# remove words that are not in the features\n",
        "vocab = vectorizer.vocabulary_\n",
        "texts_within_vocab=[]\n",
        "\n",
        "for doc in words_token:\n",
        "  document=[]\n",
        "  for word in doc:\n",
        "    if word in vocab:\n",
        "      document.append(word)\n",
        "  texts_within_vocab.append(document)\n",
        "\n",
        "# max_length of documents\n",
        "max_length = -1\n",
        "for doc in texts_within_vocab:\n",
        "  if len(doc) > max_length:\n",
        "    max_length = len(doc)\n",
        "\n",
        "training_set = np.zeros((X.shape[0],max_length,200))\n",
        "for i,doc in enumerate(texts_within_vocab):\n",
        "  for j,word in enumerate(doc):\n",
        "    word_idx=vocab[word]\n",
        "    word_tfidf = X[i,word_idx]\n",
        "    training_set[i,j+max_length-len(doc),word_idx]=word_tfidf\n",
        "print(training_set.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 313, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPcsjEdrIkOw"
      },
      "source": [
        "### Data Loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXLTtQfcIflt"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "dataset = TensorDataset(torch.from_numpy(training_set),torch.from_numpy(Y))\n",
        "train_size=int(0.8*len(dataset))\n",
        "val_size=len(dataset)-train_size\n",
        "train_dataset,val_dataset=torch.utils.data.random_split(dataset,[train_size,val_size])\n",
        "\n",
        "train_loader= DataLoader(train_dataset,shuffle=True,batch_size=batch_size)\n",
        "val_loader= DataLoader(val_dataset,shuffle=True,batch_size=batch_size)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0Ys5qblJYDK"
      },
      "source": [
        "### Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdsWGqhwNUYI",
        "outputId": "16b7146a-d636-4699-a192-6c4bedeedf84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#training and validation\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = 0.01\n",
        "clip = 5\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate,weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  train_acc, train_loss = 0.0, 0.0\n",
        "  batch_idx = 0\n",
        "\n",
        "  for batch_x, batch_y in train_loader:\n",
        "    batch_idx += 1\n",
        "    train_h = model.init_hidden(batch_size)\n",
        "    inputs, labels = batch_x.to(device,dtype=torch.float), batch_y.to(device,dtype=torch.long)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    predicted_outputs, train_h= model(inputs,train_h)\n",
        "    pred=predicted_outputs.data.max(1)[1]\n",
        "    correct = pred.eq(labels.max(1)[1]).sum().item()\n",
        "    acc =  np.true_divide(correct,len(labels))\n",
        "    loss = criterion(predicted_outputs, torch.max(labels,1)[1])\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm(model.parameters(),clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    train_acc += acc\n",
        "    print('Training epoch: %d, batch: %d, train acc: %.3f,train loss: %.3f'%(epoch+1, batch_idx, train_acc, train_loss))\n",
        "    train_loss,train_acc = 0.0, 0.0\n",
        "\n",
        "  #validation\n",
        "  model.eval()\n",
        "\n",
        "  val_acc, val_loss = 0.0, 0.0\n",
        "  val_h = model.init_hidden(batch_size)\n",
        "\n",
        "  with  torch.no_grad():\n",
        "    for batch_x,batch_y in val_loader:\n",
        "      inputs, labels = batch_x.to(device,dtype=torch.float), batch_y.to(device,dtype=torch.long)\n",
        "      predicted_outputs, val_h = model(inputs,val_h)\n",
        "      loss = criterion(predicted_outputs, torch.max(labels,1)[1])\n",
        "      pred=predicted_outputs.data.max(1)[1]\n",
        "      correct = pred.eq(labels.data.max(1)[1]).sum().item()\n",
        "      acc =  np.true_divide(correct,len(labels))\n",
        "      val_loss += loss.item()\n",
        "      val_acc += acc\n",
        "    val_loss=val_loss/len(val_loader)\n",
        "    val_acc=val_acc/len(val_loader)\n",
        "  print('\\t Validation epoch: %d, val acc: %.3f, val loss: %.3f'%(epoch+1, val_acc, val_loss))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training epoch: 1, batch: 1, train acc: 0.359,train loss: 0.954\n",
            "Training epoch: 1, batch: 2, train acc: 0.391,train loss: 0.923\n",
            "Training epoch: 1, batch: 3, train acc: 0.625,train loss: 0.688\n",
            "Training epoch: 1, batch: 4, train acc: 0.531,train loss: 0.782\n",
            "Training epoch: 1, batch: 5, train acc: 0.469,train loss: 0.845\n",
            "Training epoch: 1, batch: 6, train acc: 0.484,train loss: 0.829\n",
            "Training epoch: 1, batch: 7, train acc: 0.375,train loss: 0.938\n",
            "Training epoch: 1, batch: 8, train acc: 0.484,train loss: 0.829\n",
            "Training epoch: 1, batch: 9, train acc: 0.562,train loss: 0.751\n",
            "Training epoch: 1, batch: 10, train acc: 0.500,train loss: 0.813\n",
            "Training epoch: 1, batch: 11, train acc: 0.453,train loss: 0.860\n",
            "Training epoch: 1, batch: 12, train acc: 0.531,train loss: 0.782\n",
            "Training epoch: 1, batch: 13, train acc: 0.562,train loss: 0.751\n",
            "Training epoch: 1, batch: 14, train acc: 0.453,train loss: 0.860\n",
            "Training epoch: 1, batch: 15, train acc: 0.438,train loss: 0.876\n",
            "Training epoch: 1, batch: 16, train acc: 0.406,train loss: 0.907\n",
            "Training epoch: 1, batch: 17, train acc: 0.484,train loss: 0.829\n",
            "Training epoch: 1, batch: 18, train acc: 0.422,train loss: 0.873\n",
            "Training epoch: 1, batch: 19, train acc: 0.531,train loss: 0.782\n",
            "Training epoch: 1, batch: 20, train acc: 0.328,train loss: 0.985\n",
            "Training epoch: 1, batch: 21, train acc: 0.453,train loss: 0.860\n",
            "Training epoch: 1, batch: 22, train acc: 0.531,train loss: 0.782\n",
            "Training epoch: 1, batch: 23, train acc: 0.609,train loss: 0.704\n",
            "Training epoch: 1, batch: 24, train acc: 0.469,train loss: 0.845\n",
            "Training epoch: 1, batch: 25, train acc: 0.406,train loss: 0.907\n",
            "\t Validation epoch: 1, val acc: 0.496, val loss: 0.818\n",
            "Training epoch: 2, batch: 1, train acc: 0.422,train loss: 0.891\n",
            "Training epoch: 2, batch: 2, train acc: 0.516,train loss: 0.798\n",
            "Training epoch: 2, batch: 3, train acc: 0.484,train loss: 0.829\n",
            "Training epoch: 2, batch: 4, train acc: 0.484,train loss: 0.829\n",
            "Training epoch: 2, batch: 5, train acc: 0.500,train loss: 0.813\n",
            "Training epoch: 2, batch: 6, train acc: 0.516,train loss: 0.798\n",
            "Training epoch: 2, batch: 7, train acc: 0.469,train loss: 0.845\n",
            "Training epoch: 2, batch: 8, train acc: 0.562,train loss: 0.751\n",
            "Training epoch: 2, batch: 9, train acc: 0.438,train loss: 0.876\n",
            "Training epoch: 2, batch: 10, train acc: 0.453,train loss: 0.860\n",
            "Training epoch: 2, batch: 11, train acc: 0.484,train loss: 0.829\n",
            "Training epoch: 2, batch: 12, train acc: 0.422,train loss: 0.891\n",
            "Training epoch: 2, batch: 13, train acc: 0.547,train loss: 0.766\n",
            "Training epoch: 2, batch: 14, train acc: 0.531,train loss: 0.782\n",
            "Training epoch: 2, batch: 15, train acc: 0.484,train loss: 0.829\n",
            "Training epoch: 2, batch: 16, train acc: 0.531,train loss: 0.782\n",
            "Training epoch: 2, batch: 17, train acc: 0.438,train loss: 0.876\n",
            "Training epoch: 2, batch: 18, train acc: 0.516,train loss: 0.798\n",
            "Training epoch: 2, batch: 19, train acc: 0.547,train loss: 0.766\n",
            "Training epoch: 2, batch: 20, train acc: 0.531,train loss: 0.782\n",
            "Training epoch: 2, batch: 21, train acc: 0.453,train loss: 0.860\n",
            "Training epoch: 2, batch: 22, train acc: 0.422,train loss: 0.891\n",
            "Training epoch: 2, batch: 23, train acc: 0.516,train loss: 0.798\n",
            "Training epoch: 2, batch: 24, train acc: 0.609,train loss: 0.704\n",
            "Training epoch: 2, batch: 25, train acc: 0.516,train loss: 0.798\n",
            "\t Validation epoch: 2, val acc: 0.502, val loss: 0.811\n",
            "Training epoch: 3, batch: 1, train acc: 0.438,train loss: 0.876\n",
            "Training epoch: 3, batch: 2, train acc: 0.531,train loss: 0.782\n",
            "Training epoch: 3, batch: 3, train acc: 0.375,train loss: 0.938\n",
            "Training epoch: 3, batch: 4, train acc: 0.453,train loss: 0.860\n",
            "Training epoch: 3, batch: 5, train acc: 0.391,train loss: 0.923\n",
            "Training epoch: 3, batch: 6, train acc: 0.344,train loss: 0.970\n",
            "Training epoch: 3, batch: 7, train acc: 0.438,train loss: 0.876\n",
            "Training epoch: 3, batch: 8, train acc: 0.484,train loss: 0.829\n",
            "Training epoch: 3, batch: 9, train acc: 0.484,train loss: 0.829\n",
            "Training epoch: 3, batch: 10, train acc: 0.578,train loss: 0.735\n",
            "Training epoch: 3, batch: 11, train acc: 0.609,train loss: 0.704\n",
            "Training epoch: 3, batch: 12, train acc: 0.516,train loss: 0.798\n",
            "Training epoch: 3, batch: 13, train acc: 0.625,train loss: 0.688\n",
            "Training epoch: 3, batch: 14, train acc: 0.469,train loss: 0.845\n",
            "Training epoch: 3, batch: 15, train acc: 0.547,train loss: 0.766\n",
            "Training epoch: 3, batch: 16, train acc: 0.406,train loss: 0.907\n",
            "Training epoch: 3, batch: 17, train acc: 0.484,train loss: 0.829\n",
            "Training epoch: 3, batch: 18, train acc: 0.562,train loss: 0.751\n",
            "Training epoch: 3, batch: 19, train acc: 0.625,train loss: 0.688\n",
            "Training epoch: 3, batch: 20, train acc: 0.578,train loss: 0.735\n",
            "Training epoch: 3, batch: 21, train acc: 0.438,train loss: 0.876\n",
            "Training epoch: 3, batch: 22, train acc: 0.500,train loss: 0.813\n",
            "Training epoch: 3, batch: 23, train acc: 0.406,train loss: 0.907\n",
            "Training epoch: 3, batch: 24, train acc: 0.547,train loss: 0.766\n",
            "Training epoch: 3, batch: 25, train acc: 0.562,train loss: 0.751\n",
            "\t Validation epoch: 3, val acc: 0.502, val loss: 0.811\n",
            "Training epoch: 4, batch: 1, train acc: 0.453,train loss: 0.860\n",
            "Training epoch: 4, batch: 2, train acc: 0.594,train loss: 0.720\n",
            "Training epoch: 4, batch: 3, train acc: 0.469,train loss: 0.845\n",
            "Training epoch: 4, batch: 4, train acc: 0.516,train loss: 0.798\n",
            "Training epoch: 4, batch: 5, train acc: 0.516,train loss: 0.798\n",
            "Training epoch: 4, batch: 6, train acc: 0.578,train loss: 0.735\n",
            "Training epoch: 4, batch: 7, train acc: 0.547,train loss: 0.766\n",
            "Training epoch: 4, batch: 8, train acc: 0.531,train loss: 0.782\n",
            "Training epoch: 4, batch: 9, train acc: 0.547,train loss: 0.766\n",
            "Training epoch: 4, batch: 10, train acc: 0.469,train loss: 0.845\n",
            "Training epoch: 4, batch: 11, train acc: 0.438,train loss: 0.876\n",
            "Training epoch: 4, batch: 12, train acc: 0.484,train loss: 0.829\n",
            "Training epoch: 4, batch: 13, train acc: 0.500,train loss: 0.813\n",
            "Training epoch: 4, batch: 14, train acc: 0.578,train loss: 0.735\n",
            "Training epoch: 4, batch: 15, train acc: 0.547,train loss: 0.766\n",
            "Training epoch: 4, batch: 16, train acc: 0.516,train loss: 0.798\n",
            "Training epoch: 4, batch: 17, train acc: 0.578,train loss: 0.735\n",
            "Training epoch: 4, batch: 18, train acc: 0.438,train loss: 0.876\n",
            "Training epoch: 4, batch: 19, train acc: 0.516,train loss: 0.798\n",
            "Training epoch: 4, batch: 20, train acc: 0.469,train loss: 0.845\n",
            "Training epoch: 4, batch: 21, train acc: 0.406,train loss: 0.907\n",
            "Training epoch: 4, batch: 22, train acc: 0.422,train loss: 0.891\n",
            "Training epoch: 4, batch: 23, train acc: 0.500,train loss: 0.813\n",
            "Training epoch: 4, batch: 24, train acc: 0.359,train loss: 0.954\n",
            "Training epoch: 4, batch: 25, train acc: 0.422,train loss: 0.891\n",
            "\t Validation epoch: 4, val acc: 0.522, val loss: 0.791\n",
            "Training epoch: 5, batch: 1, train acc: 0.500,train loss: 0.813\n",
            "Training epoch: 5, batch: 2, train acc: 0.562,train loss: 0.751\n",
            "Training epoch: 5, batch: 3, train acc: 0.562,train loss: 0.751\n",
            "Training epoch: 5, batch: 4, train acc: 0.594,train loss: 0.720\n",
            "Training epoch: 5, batch: 5, train acc: 0.484,train loss: 0.829\n",
            "Training epoch: 5, batch: 6, train acc: 0.531,train loss: 0.782\n",
            "Training epoch: 5, batch: 7, train acc: 0.406,train loss: 0.907\n",
            "Training epoch: 5, batch: 8, train acc: 0.500,train loss: 0.813\n",
            "Training epoch: 5, batch: 9, train acc: 0.469,train loss: 0.845\n",
            "Training epoch: 5, batch: 10, train acc: 0.469,train loss: 0.845\n",
            "Training epoch: 5, batch: 11, train acc: 0.500,train loss: 0.813\n",
            "Training epoch: 5, batch: 12, train acc: 0.484,train loss: 0.829\n",
            "Training epoch: 5, batch: 13, train acc: 0.578,train loss: 0.735\n",
            "Training epoch: 5, batch: 14, train acc: 0.469,train loss: 0.845\n",
            "Training epoch: 5, batch: 15, train acc: 0.547,train loss: 0.766\n",
            "Training epoch: 5, batch: 16, train acc: 0.500,train loss: 0.813\n",
            "Training epoch: 5, batch: 17, train acc: 0.422,train loss: 0.891\n",
            "Training epoch: 5, batch: 18, train acc: 0.484,train loss: 0.829\n",
            "Training epoch: 5, batch: 19, train acc: 0.516,train loss: 0.798\n",
            "Training epoch: 5, batch: 20, train acc: 0.531,train loss: 0.782\n",
            "Training epoch: 5, batch: 21, train acc: 0.406,train loss: 0.907\n",
            "Training epoch: 5, batch: 22, train acc: 0.344,train loss: 0.970\n",
            "Training epoch: 5, batch: 23, train acc: 0.531,train loss: 0.782\n",
            "Training epoch: 5, batch: 24, train acc: 0.469,train loss: 0.845\n",
            "Training epoch: 5, batch: 25, train acc: 0.531,train loss: 0.782\n",
            "\t Validation epoch: 5, val acc: 0.549, val loss: 0.764\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}