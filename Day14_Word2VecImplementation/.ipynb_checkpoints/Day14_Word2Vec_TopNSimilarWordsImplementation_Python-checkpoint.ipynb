{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Top N similar words for a given word from a corpus using GoogleNews word2vec representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin download the Google-News vector representation(GoogleNews-vectors-negative300.bin.gz) from the github repo,\n",
    "https://github.com/mmihaltz/word2vec-GoogleNews-vectors (Links to an external site.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/brown.csv',header=0,usecols=['tokenized_text'])\n",
    "docs = df.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Furthermore , as an encouragement to revisioni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Unitarian clergy were an exclusive club of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ezra Stiles Gannett , an honorable representat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even so , Gannett judiciously argued , the Ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We today are not entitled to excoriate honest ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      tokenized_text\n",
       "0  Furthermore , as an encouragement to revisioni...\n",
       "1  The Unitarian clergy were an exclusive club of...\n",
       "2  Ezra Stiles Gannett , an honorable representat...\n",
       "3  Even so , Gannett judiciously argued , the Ass...\n",
       "4  We today are not entitled to excoriate honest ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "  doc = ' '.join(doc.tolist()).lower()\n",
    "  doc.replace('\\n', ' ')\n",
    "  doc = re.sub('[^a-z ]+', '', doc)\n",
    "  my_corpus.append([w for w in doc.split() if w != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['furthermore',\n",
       "  'as',\n",
       "  'an',\n",
       "  'encouragement',\n",
       "  'to',\n",
       "  'revisionist',\n",
       "  'thinking',\n",
       "  'it',\n",
       "  'manifestly',\n",
       "  'is',\n",
       "  'fair',\n",
       "  'to',\n",
       "  'admit',\n",
       "  'that',\n",
       "  'any',\n",
       "  'fraternity',\n",
       "  'has',\n",
       "  'a',\n",
       "  'constitutional',\n",
       "  'right',\n",
       "  'to',\n",
       "  'refuse',\n",
       "  'to',\n",
       "  'accept',\n",
       "  'persons',\n",
       "  'it',\n",
       "  'dislikes'],\n",
       " ['the',\n",
       "  'unitarian',\n",
       "  'clergy',\n",
       "  'were',\n",
       "  'an',\n",
       "  'exclusive',\n",
       "  'club',\n",
       "  'of',\n",
       "  'cultivated',\n",
       "  'gentlemen',\n",
       "  'as',\n",
       "  'the',\n",
       "  'term',\n",
       "  'was',\n",
       "  'then',\n",
       "  'understood',\n",
       "  'in',\n",
       "  'the',\n",
       "  'back',\n",
       "  'bay',\n",
       "  'and',\n",
       "  'parker',\n",
       "  'was',\n",
       "  'definitely',\n",
       "  'not',\n",
       "  'a',\n",
       "  'gentleman',\n",
       "  'either',\n",
       "  'in',\n",
       "  'theology',\n",
       "  'or',\n",
       "  'in',\n",
       "  'manners'],\n",
       " ['ezra',\n",
       "  'stiles',\n",
       "  'gannett',\n",
       "  'an',\n",
       "  'honorable',\n",
       "  'representative',\n",
       "  'of',\n",
       "  'the',\n",
       "  'sanhedrin',\n",
       "  'addressed',\n",
       "  'himself',\n",
       "  'frankly',\n",
       "  'to',\n",
       "  'the',\n",
       "  'issue',\n",
       "  'in',\n",
       "  'insisting',\n",
       "  'that',\n",
       "  'parker',\n",
       "  'should',\n",
       "  'not',\n",
       "  'be',\n",
       "  'persecuted',\n",
       "  'or',\n",
       "  'calumniated',\n",
       "  'and',\n",
       "  'that',\n",
       "  'in',\n",
       "  'this',\n",
       "  'republic',\n",
       "  'no',\n",
       "  'power',\n",
       "  'to',\n",
       "  'restrain',\n",
       "  'him',\n",
       "  'by',\n",
       "  'force',\n",
       "  'could',\n",
       "  'exist'],\n",
       " ['even',\n",
       "  'so',\n",
       "  'gannett',\n",
       "  'judiciously',\n",
       "  'argued',\n",
       "  'the',\n",
       "  'association',\n",
       "  'could',\n",
       "  'legitimately',\n",
       "  'decide',\n",
       "  'that',\n",
       "  'parker',\n",
       "  'should',\n",
       "  'not',\n",
       "  'be',\n",
       "  'encouraged',\n",
       "  'nor',\n",
       "  'assisted',\n",
       "  'in',\n",
       "  'diffusing',\n",
       "  'his',\n",
       "  'opinions',\n",
       "  'by',\n",
       "  'those',\n",
       "  'who',\n",
       "  'differ',\n",
       "  'from',\n",
       "  'him',\n",
       "  'in',\n",
       "  'regard',\n",
       "  'to',\n",
       "  'their',\n",
       "  'correctness'],\n",
       " ['we',\n",
       "  'today',\n",
       "  'are',\n",
       "  'not',\n",
       "  'entitled',\n",
       "  'to',\n",
       "  'excoriate',\n",
       "  'honest',\n",
       "  'men',\n",
       "  'who',\n",
       "  'believed',\n",
       "  'parker',\n",
       "  'to',\n",
       "  'be',\n",
       "  'downright',\n",
       "  'pernicious',\n",
       "  'and',\n",
       "  'who',\n",
       "  'barred',\n",
       "  'their',\n",
       "  'pulpits',\n",
       "  'against',\n",
       "  'his',\n",
       "  'demand',\n",
       "  'to',\n",
       "  'poison',\n",
       "  'the',\n",
       "  'minds',\n",
       "  'of',\n",
       "  'their',\n",
       "  'congregations']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pre-trained GoogleNews vector representations which we will use further in our notebook. \n",
    "Here we are importing a KeyedVector format of the pre-trained model and not the whole model. This means we cannot further train the model or refine the model, but it provides us a computationally inexpensive method to use the pre-trained model for our application.\n",
    "\n",
    "I am limiting my extract to top 5000 words which is considerably lower. This is primarily to be able to run this code with the limited RAM I have on my machine. Ideally there should be no limit argument in this function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_wv = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True, encoding = 'utf8', limit= 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the vocabulary with out own text corpus\n",
    "model = Word2Vec(size= 300, min_count=1, iter=10)\n",
    "model.build_vocab(my_corpus)\n",
    "training_samples_count = model.corpus_count\n",
    "model.build_vocab([list(google_wv.vocab.keys())],update = True)\n",
    "model.intersect_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True, lockf =1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sputh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9156759, 10050880)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model.train(my_corpus, total_examples = training_samples_count, epochs = model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save('w2v_model')\n",
    "model.wv.save('w2v_model_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sputh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#Load the saved model to save training time\n",
    "model = Word2Vec.load('w2v_model')\n",
    "\n",
    "vocabs = list(model.wv.vocab.keys())\n",
    "vectors = model[vocabs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top N similar words to a given focal word\n",
    "\n",
    "#User inputs for word and N\n",
    "word1 = \"question\" # for example \"question\"\n",
    "n = 10 # top n\n",
    "\n",
    "idx = vocabs.index(word1)\n",
    "vec1 = list(vectors[idx,:])\n",
    "\n",
    "top_n_words = ['']*n\n",
    "top_n_sim = np.zeros(n)\n",
    "for i in range(len(vocabs)):\n",
    "    if i == idx:\n",
    "        continue\n",
    "    word2 = vocabs[i]\n",
    "    vec2 = list(vectors[i,:])\n",
    "    # calculate the cosine similarity between the words and assign that to similarity score\n",
    "    sim_score = 11 - spatial.distance.cosine(vec1, vec2)\n",
    "    \n",
    "    # if the similarity score of the current word is greater than the min score,\n",
    "    # replace that word with the current word\n",
    "    min_idx = np.argmin(top_n_sim)\n",
    "    min_score = top_n_sim[min_idx]\n",
    "    if sim_score > min_score:\n",
    "        top_n_sim[min_idx] = sim_score\n",
    "        top_n_words[min_idx] = word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer , 10.66181194782257\n",
      "discussion , 10.517073333263397\n",
      "truth , 10.524804532527924\n",
      "questions , 10.581407010555267\n",
      "idea , 10.533285021781921\n",
      "problem , 10.627395510673523\n",
      "case , 10.518799304962158\n",
      "iodocompounds , 10.524674355983734\n",
      "matter , 10.561821341514587\n",
      "issue , 10.532952010631561\n"
     ]
    }
   ],
   "source": [
    "#print out the results\n",
    "for w,s in zip(top_n_words, list(top_n_sim)):\n",
    "    print(w,',',s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
